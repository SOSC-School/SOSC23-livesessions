{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c035b93-4cab-4cb8-8519-ce8bdded0b04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Project - Day 4 - MLFlow training of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220ce18-54f3-4d9c-9ebf-7e4a828049e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Insert MLFlow parameters\n",
    "The following cell is marked as `parameters`, you might find useful to include MLFlow usable parameters here for varying and experimenting different values for the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130e26d-a266-43a1-bb1e-53990655e143",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f8d7e-8c0f-494f-ab86-45526395081a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Excercise\n",
    "\n",
    "Based on the Training step of the project done on day 3:\n",
    "\n",
    "- train a model and store the metrics of the training process in MLFlow. e.g.:\n",
    "```python\n",
    "with mlflow.start_run(tags={\"mlflow.runName\": \"train\"}) as mlrun:\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    !pip install -q tqdm\n",
    "    from tqdm import trange\n",
    "    \n",
    "    n_epochs = 5\n",
    "    n_blocks = y_train.numblocks[0]\n",
    "    \n",
    "    for epoch in trange(n_epochs):\n",
    "        for X, y in zip(X_train.blocks, y_train.blocks):\n",
    "            losses.append(\n",
    "                (len(losses)/n_blocks, classifier.train_on_batch(X.compute(), y.compute()))\n",
    "            )\n",
    "        ls = classifier.test_on_batch(X_valid, y_valid)\n",
    "        val_losses.append(\n",
    "            (len(losses)/n_blocks,ls)\n",
    "            )\n",
    "        mlflow.log_metric(\"loss\", ls, step=int(len(losses)/n_blocks))\n",
    "\n",
    "```\n",
    "\n",
    "- store the model in MLFlow of the usage on the next step of the pipeline, e.g.:\n",
    "\n",
    "```python\n",
    "    classifier.save(\"classifier.keras\")\n",
    "    mlflow.log_artifact(\"classifier.keras\")\n",
    "    prds = classifier.predict(X_valid.compute())\n",
    "    signature = infer_signature(X_valid.compute(), prds)\n",
    "    mlflow.tensorflow.log_model(classifier, \"model\", registered_model_name=\"CYGNO_CNN\", signature=signature)\n",
    "```\n",
    "\n",
    "- store any additional plot that you find useful to track as a MLFlow artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb7c8f",
   "metadata": {},
   "source": [
    "## SOLUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## Download the training dataset from an INFN archive\n",
    "wget https://pandora.infn.it/public/269d22/dl/training_set.zip -qO $HOME/data/training_set.zip\n",
    "\n",
    "## Install the unzip utility \n",
    "#apt-get -qy install unzip\n",
    "\n",
    "## Extract the archive\n",
    "cd $HOME/data/\n",
    "unzip -qn $HOME/data/training_set.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "filenames = glob(\"/home/jovyan/data/data/export/*/*/*/*.png\")\n",
    "print (f\"Found {len(filenames)} filenames\")\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc63b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask, dask.array\n",
    "\n",
    "## See Day 2\n",
    "@dask.delayed \n",
    "def load_image(filename: str):\n",
    "    \"\"\"Wrapper function loading image as a dask.delayed\"\"\"\n",
    "    return np.asarray(Image.open(filename))\n",
    "\n",
    "## See Day 2\n",
    "def load_raw_images(filenames):\n",
    "    \"\"\"Load the images from the file paths in `filenames` into a delayed dask-array\"\"\"\n",
    "    return dask.array.stack([\n",
    "        dask.array.from_delayed(load_image(f), shape=(576, 576), dtype=np.uint8) \n",
    "        for f in filenames\n",
    "    ], axis=0)\n",
    "\n",
    "\n",
    "## Discussed in Day 1, implemented in Day 2\n",
    "def windowing(dask_image, x_min, x_max):\n",
    "    \"\"\"Maps the pixel values from the interval [x_min, x_max] to [0, 1]\"\"\"\n",
    "    return dask.array.clip((dask_image - x_min)/(x_max - x_min), 0., 1.)\n",
    "\n",
    "## Discussed in Day 1, implemented in Day 2\n",
    "def crop_center(dask_image, half_win=64):\n",
    "    \"\"\"Crop a numpy-represented image around its center, the resulting image will be a square of side 2*half_win\"\"\"\n",
    "    low, high = 576//2 - half_win, 576//2 + half_win\n",
    "    return dask_image[:,low:high, low:high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484596f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def energy_keV_from_path(filenames):\n",
    "    \"\"\"\n",
    "    Return a dask array with the energy (in keV) as obtained parsing a sequence of filenames passed \n",
    "    as an argument.\n",
    "    \"\"\"\n",
    "    return dask.array.from_array([float(re.findall(r\"/([0-9]+)_keV\", f)[0]) for f in filenames])\n",
    "\n",
    "def is_nuclear_from_path(filenames):\n",
    "    \"\"\"\n",
    "    Return an array of boolean, true for nuclear recoil, or false for electron recoils as \n",
    "    obtained parsing the list of filenames passed as an argument.\n",
    "    \"\"\"\n",
    "    return dask.array.from_array([float('NR' in re.findall(r\"/([NE]R)/\", f)) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2568639",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_filenames = np.random.RandomState(seed=42).permutation(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e79188",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 50\n",
    "validation_files = shuffled_filenames[:n_validation]\n",
    "training_files = shuffled_filenames[n_validation:]\n",
    "training_set = crop_center(windowing(load_raw_images(training_files), 60, 130))\n",
    "validation_set = crop_center(windowing(load_raw_images(validation_files), 60, 130))\n",
    "\n",
    "training_label = is_nuclear_from_path(training_files)\n",
    "validation_label = is_nuclear_from_path(validation_files) \n",
    "\n",
    "training_energy = energy_keV_from_path(training_files)\n",
    "validation_energy = energy_keV_from_path(validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input = tf.keras.Input(shape=(128,128), name=\"input\")\n",
    "hidden = tf.keras.layers.Reshape((128, 128, 1), name=\"reshape\")(input)\n",
    "hidden = tf.keras.layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-2), kernel_initializer='he_normal')(hidden)\n",
    "hidden = tf.keras.layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-2), kernel_initializer='he_normal')(hidden)\n",
    "hidden = tf.keras.layers.MaxPooling2D(2)(hidden)\n",
    "\n",
    "hidden = tf.keras.layers.Conv2D(filters=4, kernel_size=(3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-2), kernel_initializer='he_normal')(hidden)\n",
    "hidden = tf.keras.layers.Conv2D(filters=4, kernel_size=(3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-2), kernel_initializer='he_normal')(hidden)\n",
    "hidden = tf.keras.layers.MaxPooling2D(2)(hidden)\n",
    "\n",
    "hidden = tf.keras.layers.Flatten()(hidden)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal')(hidden)\n",
    "\n",
    "classifier = tf.keras.Model(input, output)\n",
    "classifier.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(3e-4))\n",
    "\n",
    "display(classifier.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "X_train = training_set.rechunk( [batch_size, None, None] )\n",
    "X_valid = validation_set.rechunk( [-1, None, None] )\n",
    "y_train = training_label.rechunk(batch_size)\n",
    "y_valid = validation_label.rechunk(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fab684",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(tags={\"mlflow.runName\": \"train\"}) as mlrun:\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    from tqdm import trange\n",
    "    \n",
    "    n_blocks = y_train.numblocks[0]\n",
    "    \n",
    "    for epoch in trange(n_epochs):\n",
    "        for X, y in zip(X_train.blocks, y_train.blocks):\n",
    "            losses.append(\n",
    "                (len(losses)/n_blocks, classifier.train_on_batch(X.compute(), y.compute()))\n",
    "            )\n",
    "        ls = classifier.test_on_batch(X_valid, y_valid)\n",
    "        val_losses.append(\n",
    "            (len(losses)/n_blocks,ls)\n",
    "            )\n",
    "        mlflow.log_metric(\"loss\", ls, step=int(len(losses)/n_blocks))\n",
    "\n",
    "    classifier.save(\"classifier.keras\")\n",
    "    mlflow.log_artifact(\"classifier.keras\")\n",
    "    prds = classifier.predict(X_valid.compute())\n",
    "    signature = infer_signature(X_valid.compute(), prds)\n",
    "    mlflow.tensorflow.log_model(classifier, \"model\", registered_model_name=\"CYGNO_CNN\", signature=signature)\n",
    "    \n",
    "\n",
    "plt.plot(*(np.array(losses).T), label=\"Training data\")\n",
    "plt.plot(*(np.array(val_losses).T), label=\"Validation data\")\n",
    "plt.title(f\"{n_epochs} epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary cross-entropy\")\n",
    "plt.legend(title=\"CYGNO-SIM\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
